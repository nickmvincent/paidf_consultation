# Failure Modes of Public AI Data Flywheels {sec-failure}

> Short first draft. Goal: name concrete ways a public data flywheel can stall or backfire, plus early signals and pragmatic mitigations to bake into "initial launch".

## Why this matters

The public AI data flywheel concepts promises some potential for compounding improvements. Usage creates data, and shared data improve can improve models (and then better models attract usage). But feedback loops amplify downsides, too.

## Two primary failure modes

There are two major failure modes that we should certainly keep top of mind when designing a flywheel.

### No good data (trivial examples, low usage)

- What happens: We set our flywheel up, but (1) our system doesn't have much use, (2) are system is mainly used by people "playing around", (3) there are a lot of users, but not enough energy or incentive to contribute. In any case, we get contributions that are sparse, short, and low‑signal.

- Mitigations: Ship strong, narrow utility first; template “valuable” examples; seed with curated gold sets; guided tasks over free‑form when early; highlight impact/attribution so contributors see outcomes; reduce cognitive/UX friction for non‑trivial submissions.

### Good data, but private actors benefit most

- What happens: The public corpus is high‑quality, but closed providers can ingest it at scale, converting shared effort into private advantage with little reciprocity. The public project bears governance and moderation costs; value accrues elsewhere.
- Why it happens: Permissive licensing without reciprocity; no access lag or rate asymmetry; weak brand/attribution; limited downstream public models using the data.
- Early signals: Prominent closed models improve on tasks overrepresented in the corpus; citations/attribution are missing; partners delay or avoid open releases while extracting data; contributor questions about “who benefits?” rise.
- Mitigations: Calibrate licenses (e.g., share‑alike terms where feasible, tiered licenses for high‑value subsets); create reciprocity agreements and data‑use covenants; introduce access symmetry (mirrors, rate limits, attribution requirements); build visible public baselines that convert data into open improvements.

Note: any endpoints that provide access to the flywheel corpus benefit from any improvements in data protection/licensing!

A third broad failure mode is loss of trust.

## Cross‑cutting failure: Loss of trust

- What happens: Even with usage and data quality, perceived misalignment (opaque data flows, surprise uses, slow removals) erodes legitimacy. People might stop using the system.
- Why it happens: Consent scope creep, unclear retention, slow or manual deletion flows, security/privacy incidents, or governance capture. There's a lot of things that could cause loss of trust.
  - Different subgroups will react to different events.
- Early signals: Rising deletion/retention requests; social media complaints about “bait‑and‑switch”; maintainers field policy questions instead of contributions; partner deals stall.
- Mitigations: Plain‑language, versioned scopes; one‑click revocation with downstream propagation commitments; published data lineage and change logs; incident response SLOs; contributor representation in governance; regular external audits.

## Other failure modes

- Governance capture? Small groups tilt curation and priorities toward incumbents.
- Data poisoning and gaming? Coordinated or subtle adversarial inputs skew training/evals.
- Privacy and security incidents? Reidentification or leaks harm users and program viability.
- Legal and policy shocks: Regulatory or IP events force takedowns and reprocessing.
- Cold start and contributor fatigue: Repeated asks without visible impact reduce motivation - not good!
- Fragmentation and forking: Incompatible schemas/licenses prevent compounding benefits.

In general, all these scenarios might show up in metrics (usage declines, lots of complaints and moderation backlog, etc.)

## Open questions

- How to structure reciprocity so open data still compounds publicly without being fully privatized downstream?
- What's the minimum viable governance model that draws from both peer production and provides good UX to users who have never engaged with peer production before?
- Which redaction/anonymization pipelines meaningfully reduce reidentification risk for multi‑modal data?
- What is the smallest set of metrics that predicts long‑run health (quality, diversity, trust) rather than just growth?
